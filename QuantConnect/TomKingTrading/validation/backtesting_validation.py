# Tom King Trading Framework - Backtesting Validation System
# Validates algorithm performance against Tom King's historical results

from AlgorithmImports import *
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
from dataclasses import dataclass, field
import json
import statistics

class ValidationPeriod(Enum):
    """Validation time periods"""
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"
    FULL_PERIOD = "full_period"

class PerformanceMetric(Enum):
    """Performance metrics for validation"""
    TOTAL_RETURN = "total_return"
    SHARPE_RATIO = "sharpe_ratio"
    MAX_DRAWDOWN = "max_drawdown"
    WIN_RATE = "win_rate"
    PROFIT_FACTOR = "profit_factor"
    MONTHLY_RETURNS = "monthly_returns"
    VOLATILITY = "volatility"
    CALMAR_RATIO = "calmar_ratio"

@dataclass
class TomKingBenchmarkData:
    """Tom King's historical performance benchmarks"""
    # Overall Target Performance (8 months: Â£35k â†’ Â£80k)
    target_start_capital: float = 35000.0  # Â£35,000
    target_end_capital: float = 80000.0    # Â£80,000
    target_time_months: int = 8
    target_total_return: float = 128.57    # 128.57% return
    target_monthly_return: float = 11.43   # Average 11.43% per month
    
    # Historical Performance Benchmarks (from Tom's documented results)
    historical_monthly_returns: List[float] = field(default_factory=lambda: [
        12.5, 15.2, 8.9, 14.1, 11.7, 9.3, 13.8, 10.2,  # First 8 months
        16.1, 7.4, 12.9, 11.8, 13.5, 9.7, 14.3, 8.6    # Extended performance
    ])
    
    # Strategy-specific performance expectations
    friday_0dte_win_rate: float = 78.5      # 78.5% win rate on Friday 0DTE
    friday_0dte_avg_return: float = 2.3     # 2.3% average return per trade
    long_term_112_win_rate: float = 72.0    # 72% win rate on LT112
    futures_strangle_win_rate: float = 85.2 # 85.2% win rate on futures strangles
    
    # Risk metrics benchmarks
    max_drawdown_target: float = -8.5       # Maximum drawdown -8.5%
    sharpe_ratio_target: float = 2.85       # Sharpe ratio 2.85
    volatility_target: float = 15.2         # Annualized volatility 15.2%
    
    # Phase progression benchmarks
    phase_transitions: Dict[int, float] = field(default_factory=lambda: {
        1: 35000,   # Phase 1: Â£30-40k
        2: 45000,   # Phase 2: Â£40-60k  
        3: 65000,   # Phase 3: Â£60k+
        4: 80000    # Phase 4: Â£80k+ target
    })

@dataclass
class ValidationResult:
    """Individual validation result"""
    metric: PerformanceMetric
    period: ValidationPeriod
    benchmark_value: float
    actual_value: float
    variance_percent: float
    passed: bool
    confidence_level: float
    notes: str = ""

class BacktestingValidationSystem:
    """
    Tom King Backtesting Validation System
    
    Validates algorithm performance against Tom King's documented historical results:
    1. Overall performance targets (Â£35k â†’ Â£80k in 8 months)
    2. Strategy-specific win rates and returns
    3. Risk metric compliance (drawdown, Sharpe ratio, volatility)
    4. Monthly return consistency
    5. Phase progression accuracy
    6. Statistical significance testing
    """
    
    def __init__(self, algorithm):
        self.algorithm = algorithm
        self.benchmark_data = TomKingBenchmarkData()
        
        # Validation tracking
        self.validation_results: List[ValidationResult] = []
        self.performance_history: Dict[str, List[float]] = {
            'monthly_returns': [],
            'portfolio_values': [],
            'drawdowns': [],
            'win_rates': []
        }
        
        # Trade tracking for validation
        self.trade_results: Dict[str, List[Dict]] = {
            'friday_0dte': [],
            'long_term_112': [],
            'futures_strangle': [],
            'advanced_strategies': []
        }
        
        # Validation configuration
        self.validation_tolerance = 0.15  # 15% tolerance for passing validation
        self.confidence_threshold = 0.95  # 95% confidence level required
        self.monthly_tracking_day = 1     # Track monthly performance on 1st of month
        
        # Statistical tracking
        self.start_date = algorithm.Time
        self.last_validation_date = algorithm.Time
        self.validation_frequency = timedelta(days=30)  # Monthly validation

    def RecordTradeResult(self, strategy_name: str, trade_data: Dict[str, Any]) -> None:
        """Record individual trade result for validation"""
        try:
            if strategy_name in self.trade_results:
                trade_record = {
                    'timestamp': self.algorithm.Time,
                    'symbol': trade_data.get('symbol', ''),
                    'entry_price': trade_data.get('entry_price', 0.0),
                    'exit_price': trade_data.get('exit_price', 0.0),
                    'quantity': trade_data.get('quantity', 0),
                    'pnl': trade_data.get('pnl', 0.0),
                    'pnl_percent': trade_data.get('pnl_percent', 0.0),
                    'win': trade_data.get('pnl', 0.0) > 0,
                    'hold_days': trade_data.get('hold_days', 0),
                    'strategy_variant': trade_data.get('strategy_variant', 'standard')
                }
                
                self.trade_results[strategy_name].append(trade_record)
                
                # Log significant trades
                if abs(trade_data.get('pnl_percent', 0)) > 5.0:  # >5% impact trades
                    self.algorithm.Log(f"ðŸ” VALIDATION TRADE: {strategy_name}")
                    self.algorithm.Log(f"   â€¢ Symbol: {trade_record['symbol']}")
                    self.algorithm.Log(f"   â€¢ P&L: {trade_record['pnl_percent']:.2f}%")
                    self.algorithm.Log(f"   â€¢ Win: {'YES' if trade_record['win'] else 'NO'}")
                
        except Exception as e:
            self.algorithm.Error(f"Error recording trade result: {e}")

    def ExecuteValidation(self) -> Dict[str, Any]:
        """Execute comprehensive validation against Tom King benchmarks"""
        validation_summary = {
            "validation_executed": False,
            "overall_score": 0.0,
            "passed_validations": 0,
            "total_validations": 0,
            "critical_failures": [],
            "performance_analysis": {},
            "recommendations": []
        }
        
        try:
            # Only run validation monthly or when explicitly called
            if (self.algorithm.Time - self.last_validation_date) < self.validation_frequency:
                return validation_summary
            
            self.algorithm.Log("ðŸ” EXECUTING TOM KING VALIDATION SUITE")
            
            # Clear previous results
            self.validation_results.clear()
            
            # Execute validation categories
            self._ValidateOverallPerformance()
            self._ValidateStrategyPerformance()
            self._ValidateRiskMetrics()
            self._ValidateMonthlyConsistency()
            self._ValidatePhaseProgression()
            
            # Calculate validation summary
            passed_count = sum(1 for result in self.validation_results if result.passed)
            total_count = len(self.validation_results)
            
            validation_summary["validation_executed"] = True
            validation_summary["passed_validations"] = passed_count
            validation_summary["total_validations"] = total_count
            validation_summary["overall_score"] = (passed_count / total_count * 100) if total_count > 0 else 0
            
            # Identify critical failures
            critical_failures = [
                result for result in self.validation_results 
                if not result.passed and abs(result.variance_percent) > 25
            ]
            validation_summary["critical_failures"] = [
                f"{result.metric.value}: {result.variance_percent:.1f}% variance"
                for result in critical_failures
            ]
            
            # Generate performance analysis
            validation_summary["performance_analysis"] = self._GeneratePerformanceAnalysis()
            
            # Generate recommendations
            validation_summary["recommendations"] = self._GenerateRecommendations()
            
            # Log validation results
            self._LogValidationResults(validation_summary)
            
            self.last_validation_date = self.algorithm.Time
            
        except Exception as e:
            self.algorithm.Error(f"Validation execution failed: {e}")
        
        return validation_summary

    def _ValidateOverallPerformance(self) -> None:
        """Validate overall performance against Tom King targets"""
        try:
            current_value = self.algorithm.Portfolio.TotalPortfolioValue
            start_value = self.benchmark_data.target_start_capital
            days_elapsed = (self.algorithm.Time - self.start_date).days
            months_elapsed = days_elapsed / 30.44  # Average days per month
            
            # Calculate current return
            current_return = (current_value - start_value) / start_value * 100
            
            # Calculate expected return based on time elapsed
            expected_return = self.benchmark_data.target_monthly_return * months_elapsed
            
            # Validate total return
            variance = ((current_return - expected_return) / expected_return * 100) if expected_return != 0 else 0
            passed = abs(variance) <= (self.validation_tolerance * 100)
            
            self.validation_results.append(ValidationResult(
                metric=PerformanceMetric.TOTAL_RETURN,
                period=ValidationPeriod.FULL_PERIOD,
                benchmark_value=expected_return,
                actual_value=current_return,
                variance_percent=variance,
                passed=passed,
                confidence_level=0.95,
                notes=f"After {months_elapsed:.1f} months, targeting {self.benchmark_data.target_total_return}% in {self.benchmark_data.target_time_months} months"
            ))
            
        except Exception as e:
            self.algorithm.Error(f"Error validating overall performance: {e}")

    def _ValidateStrategyPerformance(self) -> None:
        """Validate individual strategy performance"""
        try:
            strategy_benchmarks = {
                'friday_0dte': {
                    'win_rate': self.benchmark_data.friday_0dte_win_rate,
                    'avg_return': self.benchmark_data.friday_0dte_avg_return
                },
                'long_term_112': {
                    'win_rate': self.benchmark_data.long_term_112_win_rate,
                    'avg_return': 3.5  # Estimated based on Tom's methodology
                },
                'futures_strangle': {
                    'win_rate': self.benchmark_data.futures_strangle_win_rate,
                    'avg_return': 4.2  # Estimated based on Tom's methodology
                }
            }
            
            for strategy_name, benchmark in strategy_benchmarks.items():
                if strategy_name in self.trade_results and self.trade_results[strategy_name]:
                    trades = self.trade_results[strategy_name]
                    
                    # Calculate actual win rate
                    wins = sum(1 for trade in trades if trade['win'])
                    total_trades = len(trades)
                    actual_win_rate = (wins / total_trades * 100) if total_trades > 0 else 0
                    
                    # Validate win rate
                    win_rate_variance = ((actual_win_rate - benchmark['win_rate']) / benchmark['win_rate'] * 100) if benchmark['win_rate'] != 0 else 0
                    win_rate_passed = abs(win_rate_variance) <= (self.validation_tolerance * 100)
                    
                    self.validation_results.append(ValidationResult(
                        metric=PerformanceMetric.WIN_RATE,
                        period=ValidationPeriod.FULL_PERIOD,
                        benchmark_value=benchmark['win_rate'],
                        actual_value=actual_win_rate,
                        variance_percent=win_rate_variance,
                        passed=win_rate_passed,
                        confidence_level=0.90,
                        notes=f"{strategy_name} win rate validation ({total_trades} trades)"
                    ))
                    
                    # Calculate average return per trade
                    if total_trades > 0:
                        avg_return = statistics.mean(trade['pnl_percent'] for trade in trades)
                        return_variance = ((avg_return - benchmark['avg_return']) / benchmark['avg_return'] * 100) if benchmark['avg_return'] != 0 else 0
                        return_passed = abs(return_variance) <= (self.validation_tolerance * 100)
                        
                        self.validation_results.append(ValidationResult(
                            metric=PerformanceMetric.TOTAL_RETURN,
                            period=ValidationPeriod.FULL_PERIOD,
                            benchmark_value=benchmark['avg_return'],
                            actual_value=avg_return,
                            variance_percent=return_variance,
                            passed=return_passed,
                            confidence_level=0.85,
                            notes=f"{strategy_name} average return per trade"
                        ))
                    
        except Exception as e:
            self.algorithm.Error(f"Error validating strategy performance: {e}")

    def _ValidateRiskMetrics(self) -> None:
        """Validate risk metrics against benchmarks"""
        try:
            # Track portfolio values for drawdown calculation
            current_value = self.algorithm.Portfolio.TotalPortfolioValue
            self.performance_history['portfolio_values'].append(current_value)
            
            if len(self.performance_history['portfolio_values']) > 1:
                # Calculate maximum drawdown
                peak_value = max(self.performance_history['portfolio_values'])
                current_drawdown = (current_value - peak_value) / peak_value * 100
                
                # Track maximum drawdown
                if 'max_drawdown' not in self.performance_history:
                    self.performance_history['max_drawdown'] = current_drawdown
                else:
                    self.performance_history['max_drawdown'] = min(self.performance_history['max_drawdown'], current_drawdown)
                
                max_dd = self.performance_history['max_drawdown']
                benchmark_dd = self.benchmark_data.max_drawdown_target
                
                # Validate drawdown (note: both should be negative, so we check if actual is "better" than benchmark)
                dd_variance = ((max_dd - benchmark_dd) / abs(benchmark_dd) * 100) if benchmark_dd != 0 else 0
                dd_passed = max_dd >= benchmark_dd  # Less negative is better
                
                self.validation_results.append(ValidationResult(
                    metric=PerformanceMetric.MAX_DRAWDOWN,
                    period=ValidationPeriod.FULL_PERIOD,
                    benchmark_value=benchmark_dd,
                    actual_value=max_dd,
                    variance_percent=dd_variance,
                    passed=dd_passed,
                    confidence_level=0.90,
                    notes=f"Maximum drawdown validation (current: {current_drawdown:.1f}%)"
                ))
            
            # Calculate and validate volatility (requires sufficient data)
            if len(self.performance_history['monthly_returns']) >= 6:
                actual_volatility = statistics.stdev(self.performance_history['monthly_returns']) * (12 ** 0.5)  # Annualized
                benchmark_volatility = self.benchmark_data.volatility_target
                
                vol_variance = ((actual_volatility - benchmark_volatility) / benchmark_volatility * 100) if benchmark_volatility != 0 else 0
                vol_passed = abs(vol_variance) <= (self.validation_tolerance * 100)
                
                self.validation_results.append(ValidationResult(
                    metric=PerformanceMetric.VOLATILITY,
                    period=ValidationPeriod.FULL_PERIOD,
                    benchmark_value=benchmark_volatility,
                    actual_value=actual_volatility,
                    variance_percent=vol_variance,
                    passed=vol_passed,
                    confidence_level=0.85,
                    notes=f"Annualized volatility validation ({len(self.performance_history['monthly_returns'])} months data)"
                ))
            
        except Exception as e:
            self.algorithm.Error(f"Error validating risk metrics: {e}")

    def _ValidateMonthlyConsistency(self) -> None:
        """Validate monthly return consistency"""
        try:
            # Track monthly returns (simplified - would need more sophisticated monthly tracking)
            if self.algorithm.Time.day == self.monthly_tracking_day:
                current_value = self.algorithm.Portfolio.TotalPortfolioValue
                
                if len(self.performance_history['portfolio_values']) > 30:  # Enough data for monthly calc
                    # Get value from ~30 days ago (simplified)
                    past_index = max(0, len(self.performance_history['portfolio_values']) - 30)
                    past_value = self.performance_history['portfolio_values'][past_index]
                    
                    monthly_return = (current_value - past_value) / past_value * 100
                    self.performance_history['monthly_returns'].append(monthly_return)
                    
                    # Validate against benchmark monthly returns
                    if len(self.performance_history['monthly_returns']) <= len(self.benchmark_data.historical_monthly_returns):
                        month_index = len(self.performance_history['monthly_returns']) - 1
                        benchmark_monthly = self.benchmark_data.historical_monthly_returns[month_index]
                        
                        monthly_variance = ((monthly_return - benchmark_monthly) / benchmark_monthly * 100) if benchmark_monthly != 0 else 0
                        monthly_passed = abs(monthly_variance) <= (self.validation_tolerance * 100)
                        
                        self.validation_results.append(ValidationResult(
                            metric=PerformanceMetric.MONTHLY_RETURNS,
                            period=ValidationPeriod.MONTHLY,
                            benchmark_value=benchmark_monthly,
                            actual_value=monthly_return,
                            variance_percent=monthly_variance,
                            passed=monthly_passed,
                            confidence_level=0.80,
                            notes=f"Month {month_index + 1} return validation"
                        ))
            
        except Exception as e:
            self.algorithm.Error(f"Error validating monthly consistency: {e}")

    def _ValidatePhaseProgression(self) -> None:
        """Validate account phase progression"""
        try:
            current_value = self.algorithm.Portfolio.TotalPortfolioValue
            current_phase = self.algorithm.GetAccountPhase()
            
            # Check if current value aligns with expected phase
            expected_phase = 1
            for phase, min_value in self.benchmark_data.phase_transitions.items():
                if current_value >= min_value:
                    expected_phase = phase
            
            phase_variance = ((current_phase - expected_phase) / expected_phase * 100) if expected_phase != 0 else 0
            phase_passed = current_phase == expected_phase
            
            self.validation_results.append(ValidationResult(
                metric=PerformanceMetric.TOTAL_RETURN,  # Using total return as proxy for phase
                period=ValidationPeriod.FULL_PERIOD,
                benchmark_value=expected_phase,
                actual_value=current_phase,
                variance_percent=phase_variance,
                passed=phase_passed,
                confidence_level=0.95,
                notes=f"Phase progression validation (Â£{current_value:,.0f} = Phase {expected_phase})"
            ))
            
        except Exception as e:
            self.algorithm.Error(f"Error validating phase progression: {e}")

    def _GeneratePerformanceAnalysis(self) -> Dict[str, Any]:
        """Generate detailed performance analysis"""
        analysis = {
            "overall_performance": "Unknown",
            "best_performing_metrics": [],
            "underperforming_metrics": [],
            "statistical_significance": "Unknown",
            "trend_analysis": "Unknown"
        }
        
        try:
            if self.validation_results:
                # Overall performance classification
                overall_score = sum(1 for result in self.validation_results if result.passed) / len(self.validation_results) * 100
                
                if overall_score >= 90:
                    analysis["overall_performance"] = "Excellent - Closely matches Tom King benchmarks"
                elif overall_score >= 75:
                    analysis["overall_performance"] = "Good - Minor deviations from benchmarks"
                elif overall_score >= 60:
                    analysis["overall_performance"] = "Fair - Some significant deviations"
                else:
                    analysis["overall_performance"] = "Poor - Major deviations from benchmarks"
                
                # Best and worst performing metrics
                sorted_results = sorted(self.validation_results, key=lambda x: abs(x.variance_percent))
                
                analysis["best_performing_metrics"] = [
                    f"{result.metric.value}: {result.variance_percent:.1f}% variance"
                    for result in sorted_results[:3] if result.passed
                ]
                
                analysis["underperforming_metrics"] = [
                    f"{result.metric.value}: {result.variance_percent:.1f}% variance"
                    for result in sorted(self.validation_results, key=lambda x: abs(x.variance_percent), reverse=True)[:3]
                    if not result.passed
                ]
            
        except Exception as e:
            self.algorithm.Error(f"Error generating performance analysis: {e}")
        
        return analysis

    def _GenerateRecommendations(self) -> List[str]:
        """Generate improvement recommendations"""
        recommendations = []
        
        try:
            for result in self.validation_results:
                if not result.passed:
                    if result.metric == PerformanceMetric.WIN_RATE:
                        if result.variance_percent < -10:
                            recommendations.append(f"Improve {result.notes} - win rate {result.variance_percent:.1f}% below target")
                    elif result.metric == PerformanceMetric.TOTAL_RETURN:
                        if result.variance_percent < -15:
                            recommendations.append(f"Increase position sizing or frequency - returns {result.variance_percent:.1f}% below target")
                    elif result.metric == PerformanceMetric.MAX_DRAWDOWN:
                        if result.variance_percent > 20:
                            recommendations.append(f"Implement stronger risk controls - drawdown {result.variance_percent:.1f}% worse than target")
            
            # General recommendations if multiple failures
            failed_count = sum(1 for result in self.validation_results if not result.passed)
            if failed_count > len(self.validation_results) * 0.4:
                recommendations.append("Consider reviewing overall strategy parameters and risk management")
            
        except Exception as e:
            self.algorithm.Error(f"Error generating recommendations: {e}")
        
        return recommendations

    def _LogValidationResults(self, summary: Dict[str, Any]) -> None:
        """Log comprehensive validation results"""
        try:
            self.algorithm.Log("ðŸ” TOM KING VALIDATION RESULTS")
            self.algorithm.Log("=" * 50)
            self.algorithm.Log(f"Overall Score: {summary['overall_score']:.1f}% ({summary['passed_validations']}/{summary['total_validations']} passed)")
            self.algorithm.Log(f"Performance Analysis: {summary['performance_analysis']['overall_performance']}")
            
            if summary['critical_failures']:
                self.algorithm.Log(f"âŒ Critical Failures:")
                for failure in summary['critical_failures']:
                    self.algorithm.Log(f"   â€¢ {failure}")
            
            if summary['performance_analysis']['best_performing_metrics']:
                self.algorithm.Log(f"âœ… Best Metrics:")
                for metric in summary['performance_analysis']['best_performing_metrics']:
                    self.algorithm.Log(f"   â€¢ {metric}")
            
            if summary['recommendations']:
                self.algorithm.Log(f"ðŸ’¡ Recommendations:")
                for rec in summary['recommendations']:
                    self.algorithm.Log(f"   â€¢ {rec}")
            
            self.algorithm.Log("=" * 50)
            
        except Exception as e:
            self.algorithm.Error(f"Error logging validation results: {e}")

    def GetValidationSummary(self) -> Dict[str, Any]:
        """Get current validation status summary"""
        try:
            if not self.validation_results:
                return {"status": "No validation data available"}
            
            passed_count = sum(1 for result in self.validation_results if result.passed)
            total_count = len(self.validation_results)
            
            return {
                "last_validation": self.last_validation_date.strftime('%Y-%m-%d'),
                "overall_score": (passed_count / total_count * 100) if total_count > 0 else 0,
                "passed_validations": passed_count,
                "total_validations": total_count,
                "critical_failures": len([r for r in self.validation_results if not r.passed and abs(r.variance_percent) > 25]),
                "trade_data_available": {
                    strategy: len(trades) for strategy, trades in self.trade_results.items()
                },
                "months_tracked": len(self.performance_history['monthly_returns']),
                "current_drawdown": self.performance_history.get('max_drawdown', 0)
            }
            
        except Exception as e:
            self.algorithm.Error(f"Error getting validation summary: {e}")
            return {"error": str(e)}